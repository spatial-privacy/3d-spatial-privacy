{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import scipy.io\n",
    "import time\n",
    "import h5py\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.colors import ListedColormap, LinearSegmentedColormap\n",
    "from mpl_toolkits.axes_grid1.axes_divider import make_axes_locatable\n",
    "\n",
    "from numpy import linalg as LA\n",
    "from scipy.spatial import Delaunay\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "#sys.path.insert(0, \"../\")\n",
    "from info3d import *\n",
    "from nn_matchers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Global parameters\n",
    "\n",
    "radius_range = np.arange(0.5,1.6,0.5)\n",
    "\n",
    "with open('point_collection/new_contiguous_point_collection.pickle','rb') as f: \n",
    "    new_contiguous_point_collection = pickle.load(f)\n",
    "\n",
    "point_collection_indices = np.arange(len(new_contiguous_point_collection))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 0: Extract the point cloud data from the OBJ files\n",
    " - store as numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Getting the path to the Raw point clouds\n",
    "\n",
    "OBJ_DIR = 'point_clouds/'\n",
    "\n",
    "# Format is a tuple of (file path, file name).\n",
    "OBJ_PATHS = [ [os.path.join(OBJ_DIR, f), f]\n",
    "                    for f in os.listdir(OBJ_DIR) ]\n",
    "\n",
    "OBJ_PATHS.remove(OBJ_PATHS[0]) # remove .DS Store file\n",
    "\n",
    "# If we desire to treat the various objects within the OBJ as different spaces,\n",
    "# we store it here.\n",
    "new_per_object_collection = []\n",
    "\n",
    "# If we desire to treat the entire OBJ as a space,\n",
    "# we store it here.\n",
    "new_per_space_collection = []\n",
    "\n",
    "# This is to combine the objects within an OBJ to slightly larger objects. \n",
    "#Roughly 2.5m radius\n",
    "combining_radius = 2.5\n",
    "\n",
    "for path, filename in OBJ_PATHS:\n",
    "    \n",
    "    pointCloudFile = open(path)#'point_cloud.obj')('SpatialMesh.obj')#\n",
    "\n",
    "    pointCloudLines = pointCloudFile.readlines()\n",
    "    \n",
    "    pointCollection = [] # object_number, vertices, vertex normals, polygons\n",
    "\n",
    "    pointCollection_no_objects = [[],[],[]] \n",
    "    prev_length = 0\n",
    "    vertices_length = 1\n",
    "    \n",
    "    for line in pointCloudLines:\n",
    "\n",
    "        if line == '\\n': continue\n",
    "\n",
    "        line_items = line.split()\n",
    "\n",
    "        if line_items[0] == 'o':\n",
    "            object_item = line_items[1].split('.')\n",
    "            object_number = int(object_item[-1])\n",
    "            pointCollection.append([object_number, [], [], []])\n",
    "            vertices_length += prev_length\n",
    "\n",
    "            #print(object_number)\n",
    "\n",
    "        if line_items[0] == 'v':\n",
    "            x = line_items[1]\n",
    "            y = line_items[2]\n",
    "            z = line_items[3]\n",
    "            if len(line_items)> 4:\n",
    "                c = line_items[4]\n",
    "                if float(c) > 0.85:\n",
    "                    pointCollection[object_number-1][1].append([float(x),float(y),float(z),float(c)])\n",
    "                    pointCollection_no_objects[0].append([float(x),float(y),float(z),float(c)])\n",
    "            else:\n",
    "                pointCollection[object_number-1][1].append([float(x),float(y),float(z)])\n",
    "                pointCollection_no_objects[0].append([float(x),float(y),float(z)])\n",
    "            prev_length = len(pointCollection[object_number-1][1])\n",
    "\n",
    "\n",
    "        if line_items[0] == 'vn':\n",
    "            x = line_items[1]\n",
    "            y = line_items[2]\n",
    "            z = line_items[3]\n",
    "            pointCollection[object_number-1][2].append([float(x),float(y),float(z)])\n",
    "            pointCollection_no_objects[1].append([float(x),float(y),float(z)])\n",
    "\n",
    "        if line_items[0] == 'f':\n",
    "            p1 = int(line_items[1].split('//')[0])\n",
    "            p2 = int(line_items[2].split('//')[0])\n",
    "            p3 = int(line_items[3].split('//')[0])\n",
    "            pointCollection[object_number-1][3].append([p1-vertices_length,\n",
    "                                                        p2-vertices_length,\n",
    "                                                        p3-vertices_length])\n",
    "            pointCollection_no_objects[2].append([p1-1,p2-1,p3-1])        \n",
    "\n",
    "    pointCloudFile.close\n",
    "    print(filename)\n",
    "    #print(\" Length of point collection\",len(pointCollection[1][1]))\n",
    "    print(\" Total number of point collections\",len(pointCollection))\n",
    "    \n",
    "    #Combining objects: 1. Getting centroids\n",
    "    centroids = []\n",
    "\n",
    "    for object_name, pointCloud, _vn, triangles in pointCollection:\n",
    "        pointCloud = np.asarray(pointCloud)\n",
    "        triangles = np.asarray(triangles)#-vertices_length\n",
    "        normals = np.asarray(_vn)\n",
    "        \n",
    "        centroids.append([object_name,\n",
    "              np.mean(pointCloud[:,0]), # x-axis\n",
    "              np.mean(pointCloud[:,2]), # z-axis\n",
    "             ])\n",
    "        \n",
    "    centroids = np.asarray(centroids)\n",
    "    remaining_centroids = np.copy(centroids)\n",
    "    \n",
    "    combinedPointCollection = []\n",
    "    count = 1\n",
    "    \n",
    "    while(remaining_centroids.size!=0):\n",
    "        #print(\"  \",remaining_centroids[0,0],remaining_centroids.shape)\n",
    "        #focusing on the first remaining collection, i.e. through its centroid\n",
    "        collection = np.where(LA.norm(remaining_centroids[:,1:] - remaining_centroids[0,1:], axis = 1)<combining_radius)[0]\n",
    "        \n",
    "        #print(\"  \",collection)\n",
    "        \n",
    "        pointCloud = np.asarray(pointCollection[int(remaining_centroids[collection[0],0]-1)][1])\n",
    "        normals = np.asarray(pointCollection[int(remaining_centroids[collection[0],0])-1][2])\n",
    "        triangles = np.asarray(pointCollection[int(remaining_centroids[collection[0],0])-1][3])\n",
    "\n",
    "        for index in collection[1:]:\n",
    "            \n",
    "            n_pointCloud = np.asarray(pointCollection[int(remaining_centroids[index,0])-1][1])\n",
    "            n_normals = np.asarray(pointCollection[int(remaining_centroids[index,0])-1][2])\n",
    "            n_triangles = np.asarray(pointCollection[int(remaining_centroids[index,0])-1][3])+len(pointCloud)\n",
    "\n",
    "            pointCloud = np.concatenate((pointCloud,n_pointCloud),0)\n",
    "            normals = np.concatenate((normals,n_normals),0)\n",
    "            triangles = np.concatenate((triangles,n_triangles),0)\n",
    "        \n",
    "        new_per_object_collection.append([\n",
    "            filename+str(count),\n",
    "            pointCloud,\n",
    "            normals,\n",
    "            triangles])\n",
    "        \n",
    "        combinedPointCollection.append([\n",
    "            pointCloud,\n",
    "            normals,\n",
    "            triangles])\n",
    "            \n",
    "        count += 1\n",
    "        remaining_centroids = np.delete(remaining_centroids,collection,axis=0)\n",
    "        #print(\"   post\",remaining_centroids.shape)\n",
    "        \n",
    "    print(\" Resulting number =\",len(combinedPointCollection))\n",
    "#    new_per_object_collection.append([\n",
    "#        filename,\n",
    "#        combinedPointCollection\n",
    "#    ])\n",
    "    \n",
    "    new_per_space_collection.append([\n",
    "        filename,\n",
    "        np.asarray(pointCollection_no_objects[0]),\n",
    "        np.asarray(pointCollection_no_objects[1]),\n",
    "        np.asarray(pointCollection_no_objects[2])\n",
    "    ])\n",
    "\n",
    "print(len(new_per_object_collection),\"total objects.\")\n",
    "print(len(new_per_space_collection),\"total spaces.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Combining the point position information with their normal vectors.\n",
    "\n",
    "new_contiguous_point_collection = []\n",
    "\n",
    "for object_name, pointCloud, vn, triangles in new_per_space_collection:\n",
    "    new_contiguous_point_collection.append([\n",
    "        object_name, \n",
    "        np.concatenate((pointCloud,vn),axis = 1),\n",
    "        triangles\n",
    "    ])\n",
    "    \n",
    "sample_index = np.random.choice(len(new_contiguous_point_collection))\n",
    "print(\"Sample:\",\n",
    "      new_contiguous_point_collection[sample_index][0],\n",
    "      new_contiguous_point_collection[sample_index][1].shape, # shape of the point cloud array\n",
    "      new_contiguous_point_collection[sample_index][2].shape # shape of the triangle array\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Uncomment below if you want to overwrite the exisiting point collection.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "with open('point_collection/new_contiguous_point_collection.pickle','wb') as f: \n",
    "    pickle.dump(new_contiguous_point_collection,f)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "fig=plt.figure()#figsize=(20, 14.5))\n",
    "\n",
    "sample_name = new_contiguous_point_collection[sample_index][0]\n",
    "sample_pointCloud = new_contiguous_point_collection[sample_index][1]\n",
    "sample_triangles = new_contiguous_point_collection[sample_index][2]\n",
    "\n",
    "print(sample_name,sample_pointCloud.shape,sample_triangles.shape)\n",
    "\n",
    "ax0 = fig.add_subplot(1,1,1, projection='3d')\n",
    "ax0.set_xlabel('X')\n",
    "ax0.set_ylabel('Z')\n",
    "ax0.set_zlabel('Y')\n",
    "ax0.set_zlim(-1.5,1)\n",
    "#ax0.set_title('Dense Point Cloud')\n",
    "\n",
    "X = sample_pointCloud[:,0]\n",
    "Y = sample_pointCloud[:,1]\n",
    "Z = sample_pointCloud[:,2]\n",
    "\n",
    "ax0.set_xlim(min(X), max(X))\n",
    "ax0.set_ylim(min(-Z), max(-Z))\n",
    "\n",
    "try:\n",
    "    ax0.scatter(\n",
    "        X,-Z,Y,\n",
    "        alpha = 0.3\n",
    "    )\n",
    "\n",
    "    ax0.plot_trisurf(\n",
    "        X, -Z, Y, \n",
    "        triangles=sample_triangles,\n",
    "        cmap=plt.cm.Spectral\n",
    "        #alpha = 0.5\n",
    "    )\n",
    "\n",
    "except Exception as ex:\n",
    "    print(ex)\n",
    "    pass\n",
    "\n",
    "#plt.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 0.1 Getting the NNs for creating samples for testing\n",
    "\n",
    "NOTE! This step is only essential for creating sample sapces for testing. There are prepared samples at testing_samples/*. Thus, you may skip this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For better testing, use bigger nn_range. \n",
    "# Originally, we used nn_range = 20,000 to reliably produce partial spaces of up to 3 meters.\n",
    "\n",
    "nn_range = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    # Create target Directory\n",
    "    os.mkdir('hd5')\n",
    "    print(\"Directory \" , 'hd5' ,  \" Created \") \n",
    "except FileExistsError:\n",
    "    print(\"Directory \" , 'hd5' ,  \" already exists\")\n",
    "\n",
    "for object_name, pointCloud, triangles in new_contiguous_point_collection:\n",
    "    \n",
    "    t0 = time.time()\n",
    "    \n",
    "    print(object_name, np.asarray(pointCloud).shape, np.asarray(triangles).shape)\n",
    "    \n",
    "    nbrs = NearestNeighbors(n_neighbors=min(nn_range, len(pointCloud)-1), algorithm='kd_tree').fit(pointCloud[:,:3])\n",
    "    print(\"  Done in getting tree {:.3f} seconds.\".format(time.time()-t0))\n",
    "\n",
    "    distances, indices = nbrs.kneighbors(pointCloud[:,:3])\n",
    "\n",
    "    print(\"  NN shape: {}, Mean fartherst NN distance = {:3f} m (±{:3f})\".format(\n",
    "        distances.shape,\n",
    "        np.mean(np.max(distances,axis=1)),\n",
    "        np.std(np.max(distances,axis=1))\n",
    "    ))\n",
    "    \n",
    "    with h5py.File('hd5/{}_{}_nn.h5'.format(object_name, nn_range), 'w') as f:\n",
    "        f.create_dataset('distances', data=distances)\n",
    "        f.create_dataset('indices', data=indices)\n",
    "\n",
    "    print(\"  Done in getting {}nn {:.3f} seconds.\".format(nn_range,time.time()-t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Checking the maximum reliable radius of partial spaces that can be produced with the current NNs.\n",
    "\"\"\"\n",
    "\n",
    "range_of_radii = []\n",
    "\n",
    "for object_name, pointCloud, triangles in new_contiguous_point_collection:\n",
    "\n",
    "    try:\n",
    "        with h5py.File('hd5/{}_{}_nn.h5'.format(object_name,nn_range), 'r') as f:\n",
    "            distances = f['distances'][:]\n",
    "\n",
    "    except:\n",
    "        print(\"Didn't get HD5 file for\", object_name,nn_range)\n",
    "        continue\n",
    "\n",
    "    #extracted_distances = np.copy(distances)\n",
    "    #print(np.mean(distances[:,-1]),np.std(distances[:,-1]))\n",
    "    \n",
    "    range_of_radii.append([\n",
    "        np.mean(distances[:,-1]),\n",
    "        np.std(distances[:,-1])\n",
    "    ])\n",
    "\n",
    "range_of_radii = np.asarray(range_of_radii)\n",
    "\n",
    "print(\"These created NNs is only reliable up to partial spaces of size {:.3f}m (±{:.3f}).\".format(\n",
    "    np.mean(range_of_radii[:,0]),\n",
    "    np.std(range_of_radii[:,0])\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Extracting the NNs we got from the previous step and storing it as a \n",
    "    list that we can access to extract partial spaces.\n",
    "\"\"\"\n",
    "extracted_nns = []\n",
    "\n",
    "for object_name, pointCloud, triangles in new_contiguous_point_collection:\n",
    "    \n",
    "    t0 = time.time()\n",
    "    \n",
    "    print(object_name, np.asarray(pointCloud).shape, np.asarray(triangles).shape)\n",
    "    \n",
    "    extracted_distances = []\n",
    "    extracted_indices = []\n",
    "    \n",
    "    with h5py.File('hd5/{}_{}_nn.h5'.format(object_name,nn_range), 'r') as f:\n",
    "        distances = f['distances']\n",
    "        indices = f['indices']\n",
    "        \n",
    "        print(\"  Done extracting from file in {:.3f} seconds.\".format(time.time()-t0))\n",
    "        \n",
    "        extracted_distances = np.copy(distances)\n",
    "        extracted_indices = np.copy(indices)\n",
    "\n",
    "    print(\"  \",extracted_distances.shape, extracted_indices.shape)\n",
    "    \n",
    "    print(\"  Done copying in {:.3f} seconds.\".format(time.time()-t0))\n",
    "    \n",
    "    extracted_nns.append([\n",
    "        object_name,\n",
    "        extracted_distances,\n",
    "        extracted_indices\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def GetPartialPointCloudOptimizedNearby(\n",
    "    pointCloud,\n",
    "    triangles,\n",
    "    radius = 1,\n",
    "    vertex = [],\n",
    "    verbose = False,\n",
    "    extract = False,\n",
    "    pointCloud_index = 0,\n",
    "    nearby = False,\n",
    "    nearby_threshold = 1.0\n",
    "):\n",
    "\n",
    "    t0 = time.time()\n",
    "    triangle_indices = np.arange(len(triangles))\n",
    "    \n",
    "    if vertex == []:\n",
    "        get_new_triangle = np.random.choice(triangle_indices)\n",
    "    #rint(\"origin-triangle index\",get_new_triangle,\"(Remember, the triangle indices can be more than the point population.)\")\n",
    "        vertex = triangles[get_new_triangle,1]\n",
    "        if verbose: print(\" Computed origin-vertex\",vertex)\n",
    "    \n",
    "    vertex = np.clip(vertex,0,len(pointCloud))\n",
    "\n",
    "    if extract:\n",
    "        nbrs = NearestNeighbors(n_neighbors=len(pointCloud)-1, algorithm='brute').fit(pointCloud[:,:3])\n",
    "        distances, indices = nbrs.kneighbors(pointCloud[:,:3])\n",
    "    \n",
    "    distances = extracted_nns[pointCloud_index][1]\n",
    "    indices = extracted_nns[pointCloud_index][2]\n",
    "    \n",
    "    if nearby:    \n",
    "        local_indexs =  indices[vertex,np.where(distances[vertex]<(radius*nearby_threshold))[0]]\n",
    "        nearby_vertex = np.random.choice(local_indexs)\n",
    "        vertex = nearby_vertex\n",
    "        \n",
    "    original_vertex = pointCloud[vertex]\n",
    "    if verbose: print(\"   \",original_vertex[:3],vertex,pointCloud[vertex,:3])\n",
    "    # makes sure that we don't get a point beyond the pC size\n",
    "    # 1.b\n",
    "    # list EVERYTHING, then update one-by-one\n",
    "\n",
    "    #t1 = time.time()\n",
    "    #print(\"  GetPartialPointCloud: Done getting nearest neighbors {:.3f} \".format(t1-t0))\n",
    "    \n",
    "    partial_pointcloud = []\n",
    "    partial_triangles = []\n",
    "    \n",
    "    #while len(triangle_indices)>0:\n",
    "    t0 = time.time()\n",
    "    # Get a starting vertex\n",
    "    \n",
    "\n",
    "    # Get the acceptable neighbors of the chosen point\n",
    "    acceptable_point_neighbors = indices[vertex,np.where(distances[vertex]<radius)[0]]\n",
    "    depletable_triangles = np.copy(triangles)\n",
    "    #print(len(acceptable_point_neighbors),\"neighbors\")\n",
    "    acceptable_neighbor_distances = distances[vertex,np.where(distances[vertex]<radius)[0]]\n",
    "    \n",
    "    #t2 = time.time()\n",
    "    #print(\"  GetPartialPointCloud: Done getting acceptable neighbors {:.3f} \".format(t2-t1))\n",
    "\n",
    "    # While distance of points to be addded are less than 1,\n",
    "    # get the indices of the connected items.\n",
    "    prev_length = 0\n",
    "    stopcount = 0\n",
    "\n",
    "    # Get all triangles with the index in the neighbor list\n",
    "    for v in acceptable_point_neighbors:    \n",
    "        included_triangle_indices = np.concatenate((\n",
    "            np.where(depletable_triangles[:,0]==v)[0],\n",
    "            np.where(depletable_triangles[:,1]==v)[0],\n",
    "            np.where(depletable_triangles[:,2]==v)[0]),0)\n",
    "\n",
    "        local_triangles = depletable_triangles[np.unique(included_triangle_indices)]\n",
    "        depletable_triangles = np.delete(depletable_triangles,np.unique(included_triangle_indices),0)\n",
    "\n",
    "        if len(partial_triangles) > 0:\n",
    "            partial_triangles = np.concatenate((partial_triangles,local_triangles),0)\n",
    "            partial_triangles = np.asarray(partial_triangles)\n",
    "        else:\n",
    "            partial_triangles = local_triangles\n",
    "\n",
    "    included_vertices = np.unique(partial_triangles.flatten('C'))\n",
    "    index_list = []\n",
    "    \n",
    "    #t3 = time.time()\n",
    "    #print(\"  GetPartialPointCloud: Done getting vertices {:.3f} \".format(t3 - t2))\n",
    "\n",
    "    for in_vertex in included_vertices:\n",
    "        if in_vertex in acceptable_point_neighbors:\n",
    "            # before adding to the partial lists, check if in the acceptable neighbor list.\n",
    "\n",
    "            partial_pointcloud.append(pointCloud[in_vertex])\n",
    "            #depletable_neighbors = np.delete(depletable_neighbors,np.where(depletable_neighbors==in_vertex),0)\n",
    "            index_list.append(in_vertex)\n",
    "            np.place(partial_triangles,partial_triangles==in_vertex,len(partial_pointcloud)-1)\n",
    "        else:\n",
    "            # if not, remove associated triangles\n",
    "            partial_triangles = np.delete(partial_triangles,np.unique(np.where(partial_triangles==in_vertex)[0]),0)\n",
    "\n",
    "    #t4 = time.time()\n",
    "    #print(\"  GetPartialPointCloud: Done getting triangles {:.3f} \".format(t4 - t3))\n",
    "    \n",
    "    partial_pointcloud = np.asarray(partial_pointcloud)\n",
    "   \n",
    "    return partial_pointcloud, partial_triangles, original_vertex, vertex\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Compute the descriptors from the extracted point cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "descriptors = []\n",
    "\n",
    "for object_name, pointCloud, triangles in new_contiguous_point_collection:\n",
    "            \n",
    "    t0 = time.time()\n",
    "    try:\n",
    "        t_descriptors, t_keypoints, t_d_c = getSpinImageDescriptors(\n",
    "            pointCloud,\n",
    "            down_resolution = 5,\n",
    "            cylindrical_quantization = [4,5]\n",
    "        )\n",
    "        #print(\"Got the true descriptors\",t_descriptors.shape,t_keypoints.shape)\n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "        print(\"Error getting the true descriptors of\",object_name,\"with size\",pointCloud.shape)\n",
    "\n",
    "\n",
    "    print(\"Done with\",object_name,\"in\",time.time()-t0,\"seconds.\")\n",
    "    print(\" \",pointCloud.shape,triangles.shape)\n",
    "    print(\" \",t_keypoints.shape,t_descriptors.shape)\n",
    "        \n",
    "    descriptors.append([\n",
    "        object_name,\n",
    "        t_descriptors,\n",
    "        t_keypoints,\n",
    "        t_d_c\n",
    "    ])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Uncomment below if you want to overwrite the exisiting descriptors.\n",
    "\n",
    "\"\"\"\n",
    "    with open('descriptors/new_complete_res5_4by5_descriptors.pickle','wb') as f:\n",
    "        pickle.dump(descriptors,f)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2.1 Creating a synthetic set of partial spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Parameters\n",
    "\"\"\"\n",
    "# We used a radius range of 0.25 to 5.0 in increments of 0.25\n",
    "radius_range = radius_range\n",
    "\n",
    "# We used 100 for our investigation\n",
    "samples = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "NOTE!\n",
    "There is a sample set of partial spaces in testing_samples/{}_partial_point_cloud.\n",
    "\n",
    "You may run this to produce a new set with different parameters (as above).\n",
    "If you wish to run it again, make sure you create the NNs from Step 0.1.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "for radius in radius_range:\n",
    "    \n",
    "    partial_point_collection = []\n",
    "    \n",
    "    t0 = time.time()\n",
    "    \n",
    "    for i in np.arange(samples):\n",
    "\n",
    "        random_object = np.random.choice(point_collection_indices)\n",
    "\n",
    "        object_name = new_contiguous_point_collection[random_object][0]\n",
    "        pointCloud = new_contiguous_point_collection[random_object][1]\n",
    "        triangles = new_contiguous_point_collection[random_object][2]\n",
    "\n",
    "        partial_pointcloud, partial_triangles, original_vertex, _v = GetPartialPointCloudOptimizedNearby(\n",
    "            np.asarray(pointCloud),\n",
    "            np.asarray(triangles),\n",
    "            radius,\n",
    "            pointCloud_index=random_object\n",
    "        )\n",
    "        \n",
    "        partial_point_collection.append([\n",
    "            [random_object, object_name, original_vertex],\n",
    "            partial_pointcloud,\n",
    "            partial_triangles            \n",
    "        ])\n",
    "        \n",
    "        with open('testing_samples/{}_partial_point_cloud.pickle'.format(radius), 'wb') as f:\n",
    "            pickle.dump(partial_point_collection,f)\n",
    "            \n",
    "        if i % 10 == (samples-1)%10:\n",
    "            print(\"Done with {} samples for radius = {:.2f} in {:.3f} seconds\".format(i,radius,time.time() - t0))\n",
    "            t0 = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2.2 Creating a synthetic set of successive partial spaces\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Parameters\n",
    "\"\"\"\n",
    "# We used a radius range of 0.25 to 5.0 in increments of 0.25.\n",
    "radius_range = radius_range\n",
    "\n",
    "# For our work, we orignally used 50 samples with further 100 successive releases for our investigation.\n",
    "# Below are lower parameters, change as desired.\n",
    "samples = 50\n",
    "releases = 50\n",
    "\n",
    "# For demonstration purposes, we skip testing some successive samples but we still accumulate them.\n",
    "skip = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "NOTE!\n",
    "There is a sample set of partial spaces in testing_samples/{}_successive_point_cloud.\n",
    "\n",
    "You may run this to produce a new set with different parameters (as above).\n",
    "If you wish to run it again, make sure you create the NNs from Step 0.1.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "for radius in radius_range:\n",
    "        \n",
    "    successive_point_collection = []\n",
    "\n",
    "    t1 = time.time()\n",
    "\n",
    "    for i in np.arange(samples):\n",
    "\n",
    "        random_object = np.random.choice(point_collection_indices)\n",
    "\n",
    "        object_name = new_contiguous_point_collection[random_object][0]\n",
    "        pointCloud = new_contiguous_point_collection[random_object][1]\n",
    "        triangles = new_contiguous_point_collection[random_object][2]\n",
    "\n",
    "        #growing_p_pointcloud = []\n",
    "\n",
    "        growing_point_collection = []\n",
    "\n",
    "        for release in np.arange(releases):\n",
    "\n",
    "            if release == 0:\n",
    "                # For first release, as usual.\n",
    "                partial_pointcloud, partial_triangles, original_vertex, v_i = GetPartialPointCloudOptimizedNearby(\n",
    "                    np.asarray(pointCloud),\n",
    "                    np.asarray(triangles),\n",
    "                    radius,\n",
    "                    pointCloud_index=random_object,\n",
    "                    #verbose = True\n",
    "                )\n",
    "            else:\n",
    "\n",
    "                # For succeeding release, get a nearby point vertex everytime from the previous vertex\n",
    "                partial_pointcloud, partial_triangles, original_vertex, v_i = GetPartialPointCloudOptimizedNearby(\n",
    "                    np.asarray(pointCloud),\n",
    "                    np.asarray(triangles),\n",
    "                    radius = radius,\n",
    "                    vertex = previous_v_i,\n",
    "                    pointCloud_index=random_object,\n",
    "                    nearby = True,\n",
    "                    nearby_threshold=2.0\n",
    "                    #verbose = True\n",
    "                )\n",
    "                \n",
    "                #print(\"  Previous vertex\",previous_vertex[:3],previous_v_i,pointCloud[previous_v_i,:3])\n",
    "                #print(\"  New vertex\",original_vertex[:3],v_i,pointCloud[v_i,:3])                \n",
    "                \n",
    "                if LA.norm(original_vertex[:3] - previous_vertex[:3]) >= 2.0*radius:\n",
    "                    print(\"  Release: {} Warning: The distance of the succeeding vertices is {:.2f}\"\n",
    "                          .format(release,LA.norm(original_vertex[:3] - previous_vertex[:3]))\n",
    "                         )\n",
    "                    \n",
    "            previous_v_i = v_i\n",
    "            previous_vertex = original_vertex\n",
    "\n",
    "            growing_point_collection.append([\n",
    "                [random_object, object_name, original_vertex],\n",
    "                partial_pointcloud,\n",
    "                partial_triangles            \n",
    "            ])\n",
    "\n",
    "            if (release % int(releases/3) == 5) and (i % 7 == 2):\n",
    "                print(\"  Done with {} releases of sample {} for radius = {:.2f} in {:.3f} seconds\"\n",
    "                      .format(release,i,radius,time.time() - t1))\n",
    "                t1 = time.time()\n",
    "\n",
    "        successive_point_collection.append([\n",
    "            [random_object, object_name],\n",
    "            growing_point_collection\n",
    "        ])\n",
    "\n",
    "        with open('testing_samples/{}_successive_point_cloud.pickle'.format(radius), 'wb') as f:\n",
    "            pickle.dump(successive_point_collection,f)\n",
    "\n",
    "    print(\" Done with radius = {:.2f} in {:.3f} seconds\".format(radius,time.time() - t0))\n",
    "    t0 = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-3d_env] *",
   "language": "python",
   "name": "conda-env-.conda-3d_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
